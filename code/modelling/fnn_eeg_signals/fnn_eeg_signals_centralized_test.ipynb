{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics.classification as M\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "from model_architectures import FNNEegSignals\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import subprocess\n",
    "from IPython import display\n",
    "from scipy.signal import savgol_filter\n",
    "display.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to swap between CPU/GPU\n",
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# Fetch experiment configuration from disk\n",
    "with open('exp_config.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    exp_conf = json.loads(json_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = f'{exp_conf[\"data_path\"]}/centralized-{exp_conf[\"dataset\"]}'\n",
    "\n",
    "# Training Dataset\n",
    "x_train = pd.read_csv(f'{dir_data}/eeg_x_train.csv')\n",
    "y_train = pd.read_csv(f'{dir_data}/eeg_y_train.csv')\n",
    "\n",
    "# Validation Dataset\n",
    "x_val = pd.read_csv(f'{dir_data}/eeg_x_val.csv')\n",
    "y_val = pd.read_csv(f'{dir_data}/eeg_y_val.csv')\n",
    "\n",
    "# Test Dataset\n",
    "x_test = pd.read_csv(f'{dir_data}/eeg_x_test.csv')\n",
    "y_test = pd.read_csv(f'{dir_data}/eeg_y_test.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalization (Z-Score)\n",
    "# Training Set\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_sd = x_train.std() \n",
    "x_train_norm = (x_train-x_train_mean)/x_train_sd\n",
    "x_train= x_train_norm\n",
    "\n",
    "# Validation Set\n",
    "x_val_mean = x_val.mean()\n",
    "x_val_sd = x_val.std() \n",
    "x_val_norm = (x_val-x_val_mean)/x_val_sd\n",
    "x_val= x_val_norm\n",
    "\n",
    "# Testing Set\n",
    "x_test_mean = x_test.mean()\n",
    "x_test_sd = x_test.std() \n",
    "x_test_norm = (x_test-x_test_mean)/x_test_sd\n",
    "x_test= x_test_norm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training/Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vram_usage():\n",
    "    try:\n",
    "        cmd = ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits']\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            vram_used = int(result.stdout.strip())\n",
    "            return vram_used\n",
    "        else:\n",
    "            print(\"Error:\", result.stderr, flush='True')\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e, flush='True')\n",
    "    return None\n",
    "\n",
    "def train(model, x_train, y_train, x_val, y_val, exp_conf, exp_name, filepath):\n",
    "    # Get the value to scale the weights based on class imbalance\n",
    "    n_pos = torch.tensor(y_train[y_train==1].shape[0]).float()\n",
    "    n_neg = torch.tensor(y_train[y_train==0].shape[0]).float()\n",
    "    pos_weight = n_neg / n_pos\n",
    "\n",
    "    # Loss Function\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = getattr(torch.optim, exp_conf['optimizer'])\n",
    "    optimizer = optimizer(model.parameters(), lr=exp_conf['learning_rate'])\n",
    "\n",
    "    # Metrics Accumulators\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    train_sensitivities = []\n",
    "    train_specificities = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_sensitivities = []\n",
    "    val_specificities = []\n",
    "    params_best_model = None\n",
    "    epoch_best_model = None\n",
    "\n",
    "    # Define evaluation metrics settings\n",
    "    accuracy_metric = M.BinaryAccuracy(threshold=exp_conf['decision_boundary']).to(device)\n",
    "    recall_metric = M.BinaryRecall(threshold=exp_conf['decision_boundary']).to(device)\n",
    "    specificity_metric = M.BinarySpecificity(threshold=exp_conf['decision_boundary']).to(device)\n",
    "\n",
    "    # For each training epoch\n",
    "    for epoch_i in range(exp_conf['n_epochs']):\n",
    "        ## Training\n",
    "        model.train()\n",
    "        \n",
    "        # Update on progress\n",
    "        print(f'Running {exp_name}, epoch {epoch_i} of {exp_conf[\"n_epochs\"]-1}')\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # Forward Pass\n",
    "        y_hat = model(x_train)\n",
    "\n",
    "        # Compute the Loss\n",
    "        train_loss = loss_function(y_hat, y_train)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ## Compute epoch training metrics\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "        train_accuracy = accuracy_metric(y_pred, y_train)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_sensitivity = recall_metric(y_pred, y_train)\n",
    "        train_sensitivities.append(train_sensitivity)\n",
    "        train_specificity = specificity_metric(y_pred, y_train)\n",
    "        train_specificities.append(train_specificity)\n",
    "\n",
    "        ## Compute epoch evaluation metrics\n",
    "        val_results = evaluate(model, x_val, y_val, exp_conf)\n",
    "\n",
    "        # Update best model\n",
    "        prev_losses = torch.as_tensor(val_losses).to(device)\n",
    "        prev_losses_bigger = prev_losses > val_results['loss']\n",
    "        if torch.all(prev_losses_bigger==True):\n",
    "            # print(\"Look at me, I'm the model now\")\n",
    "            epoch_best_model = epoch_i\n",
    "            params_best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        val_losses.append(val_results['loss'])\n",
    "        val_accuracies.append(val_results['accuracy'])\n",
    "        val_sensitivities.append(val_results['sensitivity'])\n",
    "        val_specificities.append(val_results['specificity'])\n",
    "\n",
    "\n",
    "    # Save the best model found to disk\n",
    "    if (filepath!=None):\n",
    "        torch.save(params_best_model, filepath)\n",
    "\n",
    "    # Collect all objects in CPU\n",
    "    result = {\n",
    "        # Training\n",
    "        'train_losses': torch.as_tensor(train_losses).cpu()\n",
    "        , 'train_accuracies': torch.as_tensor(train_accuracies).cpu()\n",
    "        , 'train_sensitivities': torch.as_tensor(train_sensitivities).cpu()\n",
    "        , 'train_specificities': torch.as_tensor(train_specificities).cpu()\n",
    "        # Validation\n",
    "        , 'val_losses': torch.as_tensor(val_losses).cpu()\n",
    "        , 'val_accuracies': torch.as_tensor(val_accuracies).cpu()\n",
    "        , 'val_sensitivities': torch.as_tensor(val_sensitivities).cpu()\n",
    "        , 'val_specificities': torch.as_tensor(val_specificities).cpu()\n",
    "        , 'epoch_best_model': epoch_best_model\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "def evaluate(model, x_val, y_val, exp_conf):\n",
    "    model.eval()\n",
    "    # Get the value to scale the weights based on class imbalance\n",
    "    n_pos = torch.tensor(y_val[y_val==1].shape[0]).float()\n",
    "    n_neg = torch.tensor(y_val[y_val==0].shape[0]).float()\n",
    "    pos_weight = n_neg / n_pos\n",
    "\n",
    "    # Loss Function\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x_val)\n",
    "        loss = loss_function(y_hat, y_val)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "\n",
    "        # Define evaluation metrics settings\n",
    "        accuracy_metric = M.BinaryAccuracy(threshold=exp_conf['decision_boundary']).to(device)\n",
    "        recall_metric = M.BinaryRecall(threshold=exp_conf['decision_boundary']).to(device)\n",
    "        specificity_metric = M.BinarySpecificity(threshold=exp_conf['decision_boundary']).to(device)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_metric(y_pred, y_val)\n",
    "        sensitivity = recall_metric(y_pred, y_val)\n",
    "        specificity = specificity_metric(y_pred, y_val)\n",
    "\n",
    "        results = {'loss': loss, 'accuracy': accuracy, 'sensitivity': sensitivity, 'specificity': specificity}\n",
    "\n",
    "    return results\n",
    "\n",
    "def test(model, x_val, y_val, exp_conf):\n",
    "    model.eval()\n",
    "    # Get the value to scale the weights based on class imbalance\n",
    "    n_pos = torch.tensor(y_val[y_val==1].shape[0]).float()\n",
    "    n_neg = torch.tensor(y_val[y_val==0].shape[0]).float()\n",
    "    pos_weight = n_neg / n_pos\n",
    "    \n",
    "    # Loss Function\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x_val)\n",
    "        loss = loss_function(y_hat, y_val)\n",
    "        y_pred = torch.sigmoid(y_hat)\n",
    "\n",
    "        # Define test metrics settings\n",
    "        accuracy_metric = M.BinaryAccuracy(threshold=exp_conf['decision_boundary']).to(device)\n",
    "        recall_metric = M.BinaryRecall(threshold=exp_conf['decision_boundary']).to(device)\n",
    "        specificity_metric = M.BinarySpecificity(threshold=exp_conf['decision_boundary']).to(device)\n",
    "        confusion_matrix_metric = M.BinaryConfusionMatrix(threshold=exp_conf['decision_boundary']).to(device)\n",
    "        roc_metric = M.BinaryROC(thresholds=100).to(device)\n",
    "        auroc_metric = M.BinaryAUROC().to(device)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_metric(y_pred, y_val)\n",
    "        sensitivity = recall_metric(y_pred, y_val)\n",
    "        specificity = specificity_metric(y_pred, y_val)\n",
    "        confusion_matrix = confusion_matrix_metric(y_pred, y_val)\n",
    "        auroc = auroc_metric(y_pred, y_val)\n",
    "        roc_fpr, roc_tpr, roc_thresholds = roc_metric(y_pred, y_val.long())\n",
    "\n",
    "        results = {\n",
    "            'loss': loss, 'accuracy': accuracy, 'sensitivity': sensitivity, 'specificity': specificity\n",
    "            , 'confusion_matrix': confusion_matrix, 'roc_fpr': roc_fpr.cpu(), 'roc_tpr': roc_tpr.cpu()\n",
    "            , 'roc_thresholds': roc_thresholds.cpu(), 'auroc': auroc\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def compile_results(epochs_results, inference_results, exp_conf, elapsed_time, filepath):\n",
    "    exp_conf['experiment_name'] = f\"{exp_conf['experiment_name']}_{exp_conf['dataset']}\"\n",
    "    epoch_best_model = epochs_results['epoch_best_model']\n",
    "    epochs_results = {key: epochs_results[key] for key in epochs_results.keys() if key not in 'epoch_best_model'}\n",
    "\n",
    "    ## Training Results (Wide Format)\n",
    "    results_wide_df = pd.DataFrame.from_dict(epochs_results, orient='columns')\n",
    "    results_wide_df['epoch'] = [epoch for epoch in range(exp_conf['n_epochs'])]\n",
    "    results_wide_df['exp_name'] = exp_conf['experiment_name']\n",
    "\n",
    "    ## Training Results (Long Format)\n",
    "    results_long_df = pd.melt(frame=results_wide_df, id_vars=['epoch', 'exp_name'], value_vars=epochs_results.keys(), var_name='metric', value_name='value')\n",
    "    results_long_df = results_long_df.sort_values('epoch')\n",
    "\n",
    "    ## Training Summary \n",
    "    roc_dict = {key: inference_results[key].tolist() for key in ['roc_fpr', 'roc_tpr', 'roc_thresholds']}\n",
    "    exp_conf_simpl = {key: value for key, value in exp_conf.items() if key not in ['data_path', 'models_path', 'logs_path']}\n",
    "\n",
    "    summary_exp = {\n",
    "        'exp_name': [exp_conf['experiment_name']]\n",
    "        , 'exp_type': [exp_conf['experiment_type']]\n",
    "        , 'exp_resource': [exp_conf['resource']]\n",
    "        , 'exp_device': [exp_conf['device']]\n",
    "        , 'exp_configuration': [json.dumps(exp_conf_simpl)]\n",
    "        , 'elapsed_time': [elapsed_time]\n",
    "        , 'epoch_best_model': epoch_best_model\n",
    "        , 'loss': [inference_results['loss'].item()]\n",
    "        , 'accuracy': [inference_results['accuracy'].item()]\n",
    "        , 'sensitivity': [inference_results['sensitivity'].item()]\n",
    "        , 'specificity': [inference_results['specificity'].item()]\n",
    "        , 'auroc': [inference_results['auroc'].item()]\n",
    "        , 'tp': [inference_results['confusion_matrix'].flatten()[0].item()]\n",
    "        , 'fn': [inference_results['confusion_matrix'].flatten()[1].item()]\n",
    "        , 'fp': [inference_results['confusion_matrix'].flatten()[2].item()]\n",
    "        , 'tn': [inference_results['confusion_matrix'].flatten()[3].item()]\n",
    "        , 'roc_thresholds': [json.dumps(roc_dict)]\n",
    "    }\n",
    "\n",
    "    exp_summary_df = pd.DataFrame.from_dict(summary_exp, orient='columns')\n",
    "\n",
    "    # Persist experiments to disk (If path is provided)\n",
    "    if(filepath!=None):\n",
    "        filename_train_wide = f'{filepath}/results_test_wide.csv'\n",
    "        results_wide_df.to_csv(filename_train_wide, index=False, mode='a', header=not os.path.exists(filename_train_wide))\n",
    "        filename_train_long = f'{filepath}/results_test_long.csv'\n",
    "        results_long_df.to_csv(filename_train_long, index=False, mode='a', header=not os.path.exists(filename_train_long))\n",
    "        filename_summary = f'{filepath}/exp_summary_test.csv'\n",
    "        exp_summary_df.to_csv(filename_summary, index=False, mode='a', header=not os.path.exists(filename_summary))\n",
    "    \n",
    "    return {\n",
    "        'results': {'wide': results_wide_df, 'long': results_long_df}\n",
    "        , 'summary_exp': exp_summary_df\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and data\n",
    "fnn = FNNEegSignals(exp_conf['n_layers'], exp_conf['n_units'], exp_conf['perc_dropout'])\n",
    "fnn.to(device)\n",
    "\n",
    "# Load the data sets into PyTorch Tensors\n",
    "# Training Dataset\n",
    "x_train = torch.tensor(x_train.values, device=device).float()\n",
    "y_train = torch.tensor(y_train.values, device=device).float()\n",
    "# Validation Dataset\n",
    "x_val = torch.tensor(x_val.values, device=device).float()\n",
    "y_val = torch.tensor(y_val.values, device=device).float()\n",
    "# Testing Dataset\n",
    "x_test = torch.tensor(x_test.values, device=device).float()\n",
    "y_test = torch.tensor(y_test.values, device=device).float()\n",
    "\n",
    "# Train the model and store the best model\n",
    "filepath_model = f'{exp_conf[\"models_path\"]}/{exp_conf[\"experiment_name\"]}.pt'\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epoch_results = train(fnn, x_train, y_train, x_val, y_val, exp_conf, f'{exp_conf[\"experiment_name\"]}', filepath_model)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "epoch_results = {key.replace('val', 'test') if 'val' in key else key: np.array(epoch_results[key]) for key in epoch_results.keys()}\n",
    "\n",
    "# Restore best model for inference\n",
    "model = FNNEegSignals(exp_conf['n_layers'], exp_conf['n_units'], exp_conf['perc_dropout'])\n",
    "model.load_state_dict(torch.load(filepath_model))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Final Model Evaluation\n",
    "inference_results = test(model, x_test, y_test, exp_conf)\n",
    "\n",
    "# Free GPU memory after each run\n",
    "del fnn, model, x_train, y_train, x_val, y_val, x_test, y_test\n",
    "torch.cuda.empty_cache() \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_test_results = compile_results(epoch_results, inference_results, exp_conf, elapsed_time, exp_conf['logs_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-dudlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
