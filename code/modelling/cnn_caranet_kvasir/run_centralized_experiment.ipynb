{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, time, cv2, shutil, argparse\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from utils.dataloader import get_loader, test_dataset\n",
    "from utils.utils import clip_gradient, adjust_lr, AvgMeter\n",
    "from CaraNet import caranet\n",
    "import torchinfo\n",
    "import subprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vram_usage():\n",
    "    try:\n",
    "        cmd = ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits']\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            vram_used = int(result.stdout.strip())\n",
    "            return vram_used\n",
    "        else:\n",
    "            print(\"Error:\", result.stderr, flush='True')\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e, flush='True')\n",
    "    return None\n",
    "\n",
    "# Calculate the loss\n",
    "def structure_loss(pred, mask):\n",
    "    weit = 1 + 5*torch.abs(F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask)\n",
    "    wbce = F.binary_cross_entropy_with_logits(pred, mask, reduce='none')\n",
    "    wbce = (weit*wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "    pred = torch.sigmoid(pred)\n",
    "    inter = ((pred * mask)*weit).sum(dim=(2, 3))\n",
    "    union = ((pred + mask)*weit).sum(dim=(2, 3))\n",
    "    wiou = 1 - (inter + 1)/(union - inter+1)\n",
    "    \n",
    "    return (wbce + wiou).mean()\n",
    "\n",
    "def evaluate(model, data_path):\n",
    "    model.eval()\n",
    "    image_root = '{}/images/'.format(data_path)\n",
    "    gt_root = '{}/masks/'.format(data_path)\n",
    "    test_loader = test_dataset(image_root, gt_root, 352)\n",
    "    b=0.0\n",
    "    print('[test_size]',test_loader.size)\n",
    "\n",
    "    for i in range(test_loader.size):\n",
    "        image, gt, name = test_loader.load_data()\n",
    "        \n",
    "        gt = np.asarray(gt, np.float32)\n",
    "        gt /= (gt.max() + 1e-8)\n",
    "        image = image.cuda()\n",
    "        \n",
    "        res5, res3, res2, res1 = model(image)\n",
    "\n",
    "        # Dice\n",
    "        res = res5\n",
    "        res = F.upsample(res, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "        res = res.sigmoid().data.cpu().numpy().squeeze()\n",
    "        res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "        \n",
    "        input = res\n",
    "        target = np.array(gt)\n",
    "        N = gt.shape\n",
    "        smooth = 1\n",
    "        input_flat = np.reshape(input,(-1))\n",
    "        target_flat = np.reshape(target,(-1))\n",
    " \n",
    "        intersection = (input_flat*target_flat)\n",
    "\n",
    "        dice =  (2 * intersection.sum() + smooth) / (input.sum() + target.sum() + smooth)\n",
    "        \n",
    "        a =  '{:.4f}'.format(dice)\n",
    "        a = float(a)\n",
    "        b = b + a\n",
    "\n",
    "    mdice = b/test_loader.size\n",
    "        \n",
    "    # Fixed, this should vary according to the test set size (Rather than be fixed)\n",
    "    return mdice\n",
    "\n",
    "# Trains the model for one epoch\n",
    "def train(train_loader, model, optimizer, epoch, evaluate_path, exp_config, total_step):\n",
    "    model.train()\n",
    "    # ---- multi-scale training ----\n",
    "    size_rates = [0.75, 1, 1.25]\n",
    "    loss_record1, loss_record2, loss_record3, loss_record5 = AvgMeter(), AvgMeter(), AvgMeter(), AvgMeter()\n",
    "    epoch_loss_record = AvgMeter()\n",
    "    train_mdice_record = AvgMeter()\n",
    "    \n",
    "    for i, pack in enumerate(train_loader, start=1):\n",
    "        for rate in size_rates:\n",
    "            optimizer.zero_grad()\n",
    "            # ---- data prepare ----\n",
    "            images, gts = pack\n",
    "            images = Variable(images).cuda()\n",
    "            gts = Variable(gts).cuda()\n",
    "            # ---- rescale ----\n",
    "            trainsize = int(round(exp_config['trainsize']*rate/32)*32)\n",
    "            if rate != 1:\n",
    "                images = F.upsample(images, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "                gts = F.upsample(gts, size=(trainsize, trainsize), mode='bilinear', align_corners=True)\n",
    "            # ---- forward ----\n",
    "            lateral_map_5, lateral_map_3, lateral_map_2, lateral_map_1 = model(images)\n",
    "            # ---- loss function ----\n",
    "            loss5 = structure_loss(lateral_map_5, gts)\n",
    "            loss3 = structure_loss(lateral_map_3, gts)\n",
    "            loss2 = structure_loss(lateral_map_2, gts)\n",
    "            loss1 = structure_loss(lateral_map_1, gts)\n",
    "            \n",
    "            # Structure Loss\n",
    "            loss = loss5 + loss3 + loss2 + loss1\n",
    "            \n",
    "            ## Dice Metric\n",
    "            batch_dice = []\n",
    "\n",
    "            # For each ground truth mask in this batch\n",
    "            for gt_id in range(0, len(gts)):\n",
    "                gt = gts[gt_id].cpu()\n",
    "\n",
    "                gt = np.asarray(gt, np.float32)\n",
    "                gt /= (gt.max() + 1e-8)\n",
    "\n",
    "                res = lateral_map_5[gt_id]\n",
    "                res = res.sigmoid().data.cpu().numpy().squeeze()\n",
    "                res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "                \n",
    "                input = res\n",
    "                target = np.array(gt)\n",
    "                N = gt.shape\n",
    "                smooth = 1\n",
    "                input_flat = np.reshape(input,(-1))\n",
    "                target_flat = np.reshape(target,(-1))\n",
    "        \n",
    "                intersection = (input_flat*target_flat)\n",
    "                \n",
    "                # Calculate the image dice metric and append it to the batch metrics\n",
    "                img_dice =  (2 * intersection.sum() + smooth) / (input.sum() + target.sum() + smooth)\n",
    "                batch_dice.append(img_dice)\n",
    "\n",
    "            # ---- backward ----\n",
    "            loss.backward()\n",
    "            clip_gradient(optimizer, exp_config['clip'])\n",
    "            optimizer.step()\n",
    "            # ---- recording loss ----\n",
    "            if rate == 1:\n",
    "                loss_record5.update(loss5.data, exp_config['batchsize'])\n",
    "                loss_record3.update(loss3.data, exp_config['batchsize'])\n",
    "                loss_record2.update(loss2.data, exp_config['batchsize'])\n",
    "                loss_record1.update(loss1.data, exp_config['batchsize'])\n",
    "                epoch_loss_record.update(loss.detach().cpu(), exp_config['batchsize'])\n",
    "                train_mdice_record.update(torch.tensor(batch_dice).mean(), exp_config['batchsize'])\n",
    "        # ---- train visualization ----\n",
    "        if i % 20 == 0 or i == total_step:\n",
    "            print('{} Epoch [{:03d}/{:03d}], Step [{:04d}/{:04d}], '\n",
    "                  ' lateral-5: [{:0.4f}], lateral-3: [{:0.4f}], lateral-2: [{:0.4f}], lateral-1: [{:0.4f}], '\n",
    "                  'train_loss: [{:0.4f}], train_mdice: [{:0.4f}]'.\n",
    "                  format(datetime.now(), epoch, exp_config['epoch'], i, total_step,\n",
    "                          loss_record5.show(),loss_record3.show(),loss_record2.show(),loss_record1.show()\n",
    "                          ,epoch_loss_record.show().numpy(), train_mdice_record.show()))\n",
    "    \n",
    "    save_path = f'{exp_config[\"models_path\"]}/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    test_mdice = evaluate(model, evaluate_path)\n",
    "\n",
    "    train_results_dict = {\n",
    "        'train_loss': ['{:.4f}'.format(epoch_loss_record.show().numpy())]\n",
    "        # , 'test_loss': [test_loss]\n",
    "        , 'train_mdice': ['{:.4f}'.format(train_mdice_record.show().numpy())]\n",
    "        , 'test_mdice': ['{:.4f}'.format(test_mdice)]\n",
    "        # , 'train_miou':\n",
    "        # , 'test_miou':\n",
    "        , 'epoch': [epoch]\n",
    "        , 'exp_name': [exp_config['exp_name']]\n",
    "    }\n",
    "\n",
    "    ## Create dataframe and log to disk\n",
    "    # Wide Format\n",
    "    train_results_df = pd.DataFrame.from_dict(train_results_dict, orient='columns')\n",
    "    output_path = f'{exp_config[\"logs_path\"]}/results_train_wide.csv'\n",
    "    train_results_df.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "\n",
    "    # Long Format\n",
    "    train_results_df_long = pd.melt(\n",
    "        frame=train_results_df, id_vars=['epoch', 'exp_name']\n",
    "        , value_vars=['train_loss', 'train_mdice', 'test_mdice']\n",
    "        , var_name='metric', value_name='value')\n",
    "    train_results_df_long = train_results_df_long.sort_values('epoch')\n",
    "    output_path = f'{exp_config[\"logs_path\"]}/results_train_long.csv'\n",
    "    train_results_df_long.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "    \n",
    "    ## Log Results to Disk\n",
    "    # Create files if they doesn't exist\n",
    "    if not os.path.exists(f'{exp_config[\"logs_path\"]}/log.txt'):\n",
    "        with open(f'{exp_config[\"logs_path\"]}/log.txt', 'w') as file:\n",
    "            file.close()\n",
    "\n",
    "    if not os.path.exists(f'{exp_config[\"logs_path\"]}/best.txt'):\n",
    "        with open(f'{exp_config[\"logs_path\"]}/best.txt', 'w') as file:\n",
    "            file.write('0')\n",
    "            file.close()\n",
    "\n",
    "    # Append the mdice metric to log file\n",
    "    with open(f'{exp_config[\"logs_path\"]}/log.txt', 'a') as file:\n",
    "        file.write(str(test_mdice) + '\\n')\n",
    "        file.close()\n",
    "    \n",
    "    # Fetch the best mdice metric recorded until now\n",
    "    fp = open(f'{exp_config[\"logs_path\"]}/best.txt', 'r')\n",
    "    best = fp.read()\n",
    "    fp.close()\n",
    "    \n",
    "    if test_mdice > float(best):\n",
    "        # Update the new best mdice in the file\n",
    "        fp = open(f'{exp_config[\"logs_path\"]}/best.txt','w')\n",
    "        fp.write(str(test_mdice))\n",
    "        fp.close()\n",
    "        # Update the new best mdice in the local variable\n",
    "        fp = open(f'{exp_config[\"logs_path\"]}/best.txt','r')\n",
    "        best = fp.read()\n",
    "        fp.close()\n",
    "        # Save the best model found until now\n",
    "        torch.save(model.state_dict(), save_path + 'CaraNet-best.pth' )\n",
    "        print('[Saving Snapshot:]', save_path + 'CaraNet-best.pth', test_mdice,'[best:]',best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch experiment configuration from disk\n",
    "with open('exp_config.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    exp_config = json.loads(json_data)\n",
    "\n",
    "# ---- build models ----\n",
    "torch.cuda.set_device(0)  # set your gpu device\n",
    "model = caranet().cuda()\n",
    "# summary_model = torchinfo.summary(model, input_size=[6, 3, 448, 448])\n",
    "# summary_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch experiment configuration from disk\n",
    "with open('exp_config.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "    exp_config = json.loads(json_data)\n",
    "\n",
    "# ---- build models ----\n",
    "torch.cuda.set_device(0)  # set your gpu device\n",
    "model = caranet().cuda()\n",
    "\n",
    "params = model.parameters()\n",
    "\n",
    "if exp_config['optimizer'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(params, exp_config['lr'])\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(params, exp_config['lr'], weight_decay = 1e-4, momentum = 0.9)\n",
    "    \n",
    "print(optimizer)\n",
    "image_root = '{}/images/'.format(exp_config['train_path'])\n",
    "gt_root = '{}/masks/'.format(exp_config['train_path'])\n",
    "\n",
    "train_loader = get_loader(image_root, gt_root, batchsize=exp_config['batchsize'], trainsize=exp_config['trainsize'], augmentation = exp_config['augmentation'])\n",
    "total_step = len(train_loader)\n",
    "\n",
    "print(\"#\"*20, \"Start Training\", \"#\"*20)\n",
    "\n",
    "train_start_time = time.time()\n",
    "for epoch in range(1, exp_config['epoch'] + 1):\n",
    "    adjust_lr(optimizer, exp_config['lr'], epoch, 0.1, 200)\n",
    "    train(train_loader, model, optimizer, epoch, exp_config['evaluate_path'], exp_config, total_step)\n",
    "    vram_used = get_vram_usage()\n",
    "    dict_memory = {\n",
    "        'epoch': [epoch]\n",
    "        , 'VRAM': [vram_used]\n",
    "    }\n",
    "\n",
    "    # print(dict_memory, flush=True\n",
    "    df_memory = pd.DataFrame.from_dict(dict_memory)\n",
    "    filename = f'{exp_config[\"logs_path\"]}/cnn_memory_centralized.csv'\n",
    "    df_memory.to_csv(filename, index=False, mode='a', header=not os.path.exists(filename))\n",
    "\n",
    "train_end_time = time.time()\n",
    "train_elapsed_time = train_end_time - train_start_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the log file and fetch the epoch with the best dice value\n",
    "with open(f'{exp_config[\"logs_path\"]}/log.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    data = np.array(contents.split('\\n'))\n",
    "    best_epoch = data.argmax()\n",
    "\n",
    "# Export experiment summary to disk\n",
    "results_df = pd.read_csv(f'{exp_config[\"logs_path\"]}/results_train_wide.csv')\n",
    "results_df = results_df[(results_df['exp_name']==exp_config['exp_name']) & (results_df['epoch']==best_epoch)]\n",
    "exp_config_simplified = {key: value for key, value in exp_config.items() \n",
    "                         if key not in ('train_path', 'test_path', 'evaluate_path', 'models_path', 'logs_path')}\n",
    "\n",
    "dict_exp_summary = {\n",
    "    'exp_name': [exp_config['exp_name']]\n",
    "    , 'exp_type': [exp_config['exp_type']]\n",
    "    , 'exp_resource': [exp_config['exp_resource']]\n",
    "    , 'exp_device': [exp_config['exp_device']]\n",
    "    , 'exp_configuration': [json.dumps(exp_config_simplified)]\n",
    "    , 'elapsed_time': [float(f'{train_elapsed_time:.4f}')]\n",
    "    , 'epoch_best_model': [best_epoch]\n",
    "    , 'train_loss_mean': [results_df['train_loss'].values[0]]\n",
    "    , 'train_mdice_mean': [results_df['train_mdice'].values[0]]\n",
    "    , 'test_mdice_mean': [results_df['test_mdice'].values[0]]\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize = 352\n",
    "best_model_path = f'{exp_config[\"models_path\"]}/CaraNet-best.pth'\n",
    "data_path = '../../../data/inputs/kvasir/test'\n",
    "logs_path = '../../../data/logs/experiments/cnn_caranet_kvasir/masks/'\n",
    "image_root = f'{data_path}/images/'\n",
    "gt_root = f'{data_path}/masks/'\n",
    "test_loader = test_dataset(image_root, gt_root, testsize)\n",
    "\n",
    "# Remove the masks directory and its contents if existing from a previous run\n",
    "if os.path.isdir(logs_path):\n",
    "    shutil.rmtree(logs_path)\n",
    "\n",
    "# Create a clean masks directory\n",
    "os.mkdir(logs_path)\n",
    "\n",
    "## Instantiate a model with the weights from the best model trained\n",
    "model = caranet()\n",
    "\n",
    "# Iterate over the stored model parameters and create a state_dict\n",
    "weights = torch.load(best_model_path)\n",
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for k, v in weights.items():\n",
    "    if 'total_ops' not in k and 'total_params' not in k:\n",
    "        name = k\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "# Load the weights in the model\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "# Perform inference with the stored model\n",
    "inference_results = evaluate(model, exp_config['test_path'])\n",
    "\n",
    "## For each image in the dataset generate its corresponding predicted mask and output to disk\n",
    "for i in range(test_loader.size):\n",
    "    image, gt, name = test_loader.load_data()\n",
    "    gt = np.asarray(gt, np.float32)\n",
    "    gt /= (gt.max() + 1e-8)\n",
    "    image = image.cuda()\n",
    "\n",
    "    # res = model(image)\n",
    "    res5, res4, res2, res1 = model(image)\n",
    "    res = res5\n",
    "    res = F.upsample(res, size=gt.shape, mode='bilinear', align_corners=False)\n",
    "    res = res.sigmoid().data.cpu().numpy().squeeze()\n",
    "    res = (res - res.min()) / (res.max() - res.min() + 1e-8)\n",
    "    \n",
    "    # misc.imsave(save_path+name, res)\n",
    "    cv2.imwrite(logs_path+name, res*255) # Multiply by 255 to be able to write to file (Images are normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_summary_df = pd.DataFrame.from_dict(dict_exp_summary, orient='columns')\n",
    "filename_summary = f'{exp_config[\"logs_path\"]}/exp_summary_train.csv'\n",
    "exp_summary_df.to_csv(filename_summary, index=False, mode='a', header=not os.path.exists(filename_summary))\n",
    "\n",
    "# Logs Clean-Up\n",
    "os.remove(f'{exp_config[\"logs_path\"]}/best.txt')\n",
    "os.remove(f'{exp_config[\"logs_path\"]}/log.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-flexperiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
