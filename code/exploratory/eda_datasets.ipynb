{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "from sys import getsizeof\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EEG Dataset EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Size\n",
    "labels_df = pd.read_csv(\"../../data/outputs/eeg-signals/data-prep/eeg_labels.csv\")\n",
    "# Flip the labels (Preictal: 1, Interictal: 0)\n",
    "labels_df['Label'] = pd.Series(np.where(labels_df['Label']==0, 1, 0)).to_frame()\n",
    "features_df = pd.read_csv(\"../../data/outputs/eeg-signals/data-prep/eeg_features.csv\")\n",
    "print(f'Size of Labels in Memory: {getsizeof(labels_df)/1e+9: .3f} GB')\n",
    "print(f'Size of Features in Memory: {getsizeof(features_df)/1e+9: .3f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation (EDA)\n",
    "# Distribution of Classes\n",
    "df_classes = labels_df.groupby('Label')['File'].count().rename('Examples').reset_index()\n",
    "df_classes['Percentage'] = round((df_classes['Examples']/labels_df.shape[0])*100, 3)\n",
    "\n",
    "# Distribution of Patients\n",
    "df_patients = labels_df.groupby('Patient')['File'].count().rename('Examples').reset_index()\n",
    "df_patients['Percentage'] = round((df_patients['Examples']/labels_df.shape[0])*100, 3)\n",
    "df_patients = df_patients.sort_values('Percentage', ascending=False)\n",
    "\n",
    "# Distribution of Classes per Patient\n",
    "df_patient_classes = labels_df.groupby(['Patient', 'Label'])['File'].count().rename('Examples').reset_index()\n",
    "df_patient_classes = pd.merge(\n",
    "    df_patient_classes\n",
    "    , df_patients.rename(columns={'Examples': 'Total Examples', 'Percentage': 'Patient Percentage'}), how='inner', left_on='Patient', right_on='Patient'\n",
    ")\n",
    "\n",
    "df_patient_classes['Percentage'] = round((df_patient_classes['Examples']/df_patient_classes['Total Examples'])*100, 3)\n",
    "df_patient_classes['Scaled Percentage'] = round(df_patient_classes['Patient Percentage']*(df_patient_classes['Percentage']/100), 3)\n",
    "\n",
    "# Distribution of Patients\n",
    "df_patients = labels_df.groupby('Patient')['File'].count().rename('Examples').reset_index()\n",
    "df_patients['Percentage'] = round((df_patients['Examples']/labels_df.shape[0])*100, 3)\n",
    "df_patients = df_patients.sort_values('Percentage', ascending=False)\n",
    "\n",
    "# Get the test set patients: This was calibrated manually to be close to 25% for the Test Set\n",
    "patients_test_set = df_patients.sample(frac=0.2, random_state=64)['Patient']\n",
    "sel_examples = df_patients[df_patients['Patient'].isin(patients_test_set)]['Examples'].sum()\n",
    "tot_examples = df_patients['Examples'].sum()\n",
    "print(f'% of Examples: {sel_examples/tot_examples: .3f}')\n",
    "\n",
    "# Get the dev and test sets\n",
    "x_test = features_df[labels_df['Patient'].isin(patients_test_set)].drop('File', axis=1)\n",
    "y_test = labels_df[labels_df['Patient'].isin(patients_test_set)]['Label'].to_frame('Labels')\n",
    "x_dev = features_df[~labels_df['Patient'].isin(patients_test_set)].drop('File', axis=1)\n",
    "y_dev = labels_df[~labels_df['Patient'].isin(patients_test_set)]['Label'].to_frame('Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Patient Class Distribution\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Percentage')\n",
    "plt.ylim(0, 100)\n",
    "bars = plt.bar(df_classes['Label'].astype('str'), df_classes['Percentage'], color=['#3C76B4', '#F28010'])\n",
    "plt.bar_label(bars)\n",
    "plt.show()\n",
    "\n",
    "# Patient & Class Distribution\n",
    "patients = df_patient_classes.sort_values('Patient Percentage', ascending=False)['Patient'].unique().astype(str)\n",
    "weight_counts = {\n",
    "    \"0\": df_patient_classes.sort_values('Patient Percentage', ascending=False)[df_patient_classes['Label']==0]['Scaled Percentage']\n",
    "    , \"1\": df_patient_classes.sort_values('Patient Percentage', ascending=False)[df_patient_classes['Label']==1]['Scaled Percentage']\n",
    "}\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "bottom = np.zeros(patients.shape[0])\n",
    "\n",
    "for boolean, weight_count in weight_counts.items():\n",
    "    p = ax.bar(patients, weight_count, width, label=boolean, bottom=bottom)\n",
    "    bottom += weight_count\n",
    "\n",
    "ax.set_title(\"Class Distribution per Patient\")\n",
    "ax.set_xlabel('Patient')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Centralized Partitioning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "x_dev = pd.read_csv('../../data/outputs/eeg-signals/data-prep/partitions/eeg-centralized/eeg_x_dev.csv')\n",
    "y_dev = pd.read_csv('../../data/outputs/eeg-signals/data-prep/partitions/eeg-centralized/eeg_y_dev.csv')\n",
    "x_test = pd.read_csv('../../data/outputs/eeg-signals/data-prep/partitions/eeg-centralized/eeg_x_test.csv')\n",
    "y_test = pd.read_csv('../../data/outputs/eeg-signals/data-prep/partitions/eeg-centralized/eeg_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prop_dev = y_dev['Label'].value_counts() / y_dev.shape[0]\n",
    "class_prop_test = y_test['Label'].value_counts() / y_test.shape[0]\n",
    "\n",
    "print(f'Dev Split Size: {y_dev.shape[0]}')\n",
    "print(f'Test Split Size: {y_test.shape[0]}')\n",
    "print('\\nDev Split Class Distribution')\n",
    "print(class_prop_dev)\n",
    "print('\\nTest Split Class Distribution')\n",
    "print(class_prop_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centralized/Federated Patient-Aware Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a dataframe summarizing the dev-test split\n",
    "# Patients selected in the random sampling (Check prep-eeg-signals.ipynb for details)\n",
    "sel_patients = [13, 35, 12, 34, 10, 32, 37, 22]\n",
    "test_patients_df = df_patient_classes[df_patient_classes['Patient'].isin(sel_patients)]\n",
    "dev_patients_df = df_patient_classes[~df_patient_classes['Patient'].isin(sel_patients)]\n",
    "\n",
    "## Get dataframe summarizing the class distribution in the dev-test splits\n",
    "dev_class_prop_df = dev_patients_df.groupby('Label')['Examples'].sum().rename('Examples').reset_index()\n",
    "dev_class_prop_df['Set'] = 'Dev Set'\n",
    "dev_class_prop_df['Percentage'] = round((dev_class_prop_df['Examples']/dev_class_prop_df['Examples'].sum())*100, 3)\n",
    "\n",
    "test_class_prop_df = test_patients_df.groupby('Label')['Examples'].sum().rename('Examples').reset_index()\n",
    "test_class_prop_df['Set'] = 'Test Set'\n",
    "test_class_prop_df['Percentage'] = round((test_class_prop_df['Examples']/test_class_prop_df['Examples'].sum())*100, 3)\n",
    "\n",
    "split_prop_df = pd.concat([dev_class_prop_df, test_class_prop_df], ignore_index=True)\n",
    "\n",
    "## Get dataframe summarizing fold class distribution\n",
    "# Patients selected in the random sampling (Check prep-eeg-signals.ipynb for details)\n",
    "patients_per_fold = {\n",
    "    1: [5, 7, 28, 36, 27, 21, 25, 24, 26, 20]\n",
    "    , 2: [14, 9, 3, 33, 2, 18, 38, 29, 31, 17]\n",
    "    , 3: [6, 1, 8, 11, 16, 19, 15, 4, 23, 30]\n",
    "}\n",
    "\n",
    "fold_prop_df = pd.DataFrame()\n",
    "\n",
    "for fold in patients_per_fold:\n",
    "    fold_patients = patients_per_fold[fold]\n",
    "    fold_n_patients = len(fold_patients)\n",
    "    fold_patients_df = dev_patients_df[dev_patients_df['Patient'].isin(fold_patients)]\n",
    "\n",
    "    fold_class_prop_df = fold_patients_df.groupby('Label')['Examples'].sum().rename('Examples').reset_index()\n",
    "\n",
    "    fold_summary_neg = {\n",
    "        'fold': fold, 'n_patients': fold_n_patients\n",
    "        , 'class': 0\n",
    "        , 'n_examples': fold_class_prop_df[fold_class_prop_df['Label']==0]['Examples'].values[0]\n",
    "    }\n",
    "\n",
    "    fold_summary_pos = {\n",
    "        'fold': fold, 'n_patients': fold_n_patients\n",
    "        , 'class': 1\n",
    "        , 'n_examples': fold_class_prop_df[fold_class_prop_df['Label']==1]['Examples'].values[0]\n",
    "    }\n",
    "\n",
    "    fold_prop_df = pd.concat([fold_prop_df, pd.DataFrame.from_dict([fold_summary_neg])], ignore_index=True)\n",
    "    fold_prop_df = pd.concat([fold_prop_df, pd.DataFrame.from_dict([fold_summary_pos])], ignore_index=True)\n",
    "\n",
    "path_output = '../../data/outputs/eeg-signals/data-prep/exploratory'\n",
    "split_prop_df.to_csv(f'{path_output}/eeg_split_summary.csv', header=True, index=False)\n",
    "fold_prop_df.to_csv(f'{path_output}/eeg_fold_summary.csv', header=True, index=False)\n",
    "\n",
    "data_dir = '../../data/outputs/eeg-signals/data-prep/partitions'\n",
    "\n",
    "train_labels = pd.read_csv(f'{data_dir}/centralized-PatientAware/eeg_y_train.csv')['Label']\n",
    "val_labels = pd.read_csv(f'{data_dir}/centralized-PatientAware/eeg_y_val.csv')['Label']\n",
    "test_labels = pd.read_csv(f'{data_dir}/centralized-PatientAware/eeg_y_test.csv')['Label']\n",
    "\n",
    "prop_class_train = train_labels.value_counts() / train_labels.shape[0]\n",
    "prop_class_val = val_labels.value_counts() / val_labels.shape[0]\n",
    "prop_class_test = test_labels.value_counts() / test_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polyp Dataset EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of centralized CNN splits\n",
    "data_dir = '../../data/inputs/kvasir'\n",
    "stats = []\n",
    "\n",
    "for data_split in os.listdir(data_dir):\n",
    "    imgs = os.listdir(f'{data_dir}/{data_split}/masks')\n",
    "\n",
    "    if(data_split in ['train', 'evaluate']):\n",
    "        imgs_kvasir = [img for img in imgs if not img.split('.')[0].isdigit()]\n",
    "        imgs_clinic = [img for img in imgs if img.split('.')[0].isdigit()]\n",
    "        total_px_kvasir = 0\n",
    "        total_px_clinic = 0\n",
    "        neg_px_kvasir = 0\n",
    "        neg_px_clinic = 0\n",
    "\n",
    "        for img in imgs_kvasir:\n",
    "            img_path = f'{data_dir}/{data_split}/masks/{img}'\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            total_pixels = image.shape[0] * image.shape[1]\n",
    "            white_pixels = cv2.countNonZero(image)\n",
    "            black_pixels = total_pixels - white_pixels\n",
    "\n",
    "            total_px_kvasir += total_pixels\n",
    "            neg_px_kvasir += black_pixels\n",
    "        \n",
    "        stats.append({'split': data_split, 'dataset': 'Kvasir', 'images': len(imgs_kvasir), 'total_px': total_px_kvasir, 'neg_px': neg_px_kvasir})\n",
    "        \n",
    "        for img in imgs_clinic:\n",
    "            img_path = f'{data_dir}/{data_split}/masks/{img}'\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            total_pixels = image.shape[0] * image.shape[1]\n",
    "            white_pixels = cv2.countNonZero(image)\n",
    "            black_pixels = total_pixels - white_pixels\n",
    "\n",
    "            total_px_clinic += total_pixels\n",
    "            neg_px_clinic += black_pixels\n",
    "\n",
    "        stats.append({'split': data_split, 'dataset': 'CVC-ClinicDB', 'images': len(imgs_clinic), 'total_px': total_px_clinic, 'neg_px': neg_px_clinic})\n",
    "    else:\n",
    "        total_px_colon= 0\n",
    "        neg_px_colon = 0\n",
    "\n",
    "        for img in imgs:\n",
    "            img_path = f'{data_dir}/{data_split}/masks/{img}'\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            total_pixels = image.shape[0] * image.shape[1]\n",
    "            white_pixels = cv2.countNonZero(image)\n",
    "            black_pixels = total_pixels - white_pixels\n",
    "\n",
    "            total_px_colon += total_pixels\n",
    "            neg_px_colon += black_pixels\n",
    "\n",
    "        stats.append({'split': data_split, 'dataset': 'CVC-ColonDB', 'images': len(imgs), 'total_px': total_px_colon, 'neg_px': neg_px_colon})\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "\n",
    "output_dir = '../../data/outputs/eda'\n",
    "stats_df.to_csv(f'{output_dir}/stats_polyp_centralized.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of federated CNN splits\n",
    "data_dir = '../../data/inputs/kvasir_federated'\n",
    "stats = []\n",
    "\n",
    "for config in os.listdir(f'{data_dir}'):\n",
    "    fl_config = config.split('_')[0]\n",
    "    config_dir = f'{data_dir}/{config}'\n",
    "\n",
    "\n",
    "    for client in os.listdir(config_dir):\n",
    "        fl_client = client.split('_')[1]\n",
    "        client_dir = f'{config_dir}/{client}/masks'\n",
    "    \n",
    "        accum_black_px = 0\n",
    "        accum_white_px = 0\n",
    "    \n",
    "        for img in os.listdir(client_dir):\n",
    "            n_imgs = len(os.listdir(client_dir))\n",
    "            img_path = f'{client_dir}/{img}'\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            total_pixels = image.shape[0] * image.shape[1]\n",
    "            white_pixels = cv2.countNonZero(image)\n",
    "            black_pixels = total_pixels - white_pixels\n",
    "            accum_black_px += black_pixels\n",
    "            accum_white_px += white_pixels\n",
    "\n",
    "        prop_black = round(accum_black_px / (accum_black_px + accum_white_px), 2)\n",
    "        prop_white = round(accum_white_px / (accum_black_px + accum_white_px), 2)\n",
    "\n",
    "        stats.append({'fl_config': fl_config, 'fl_client': fl_client, 'prop_neg': prop_black, 'prop_pos': prop_white, 'n_imgs': n_imgs})\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df = stats_df.sort_values(['fl_config', 'fl_client'])\n",
    "\n",
    "output_dir = '../../data/outputs/eda'\n",
    "stats_df.to_csv(f'{output_dir}/stats_polyp_fl.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-dudlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
